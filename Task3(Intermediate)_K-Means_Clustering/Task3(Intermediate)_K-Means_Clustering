{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO6bEIY208qOqA0bNBSYWyH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Codveda Technologies Internship\n","## Task 3: K-Means Clustering\n","**Intern:** Muhammad Sakibur Rahaman\n","\n","\n","**Dataset Used:** iris.csv\n","\n","## Objective\n","The objective of this task is to apply the K-Means clustering algorithm to the Iris dataset. K-Means is an unsupervised learning algorithm that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid). We will explore the dataset, preprocess it, determine the optimal number of clusters, and visualize the resulting clusters.\n","\n","## Importing Dependencies\n","We import the necessary Python libraries for data analysis, clustering, and visualization.\n","\n","\n","\n","\n"],"metadata":{"id":"cbx9Bl9kne_v"}},{"cell_type":"code","source":["# Importing necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"nRrMjcR_ncWP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Overview\n","We load the Iris dataset to explore its structure, data types, and check for missing values."],"metadata":{"id":"N5fpEbYdn3SH"}},{"cell_type":"code","source":["#mounting google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EsAxTvUxn40m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dataset loading\n","df = pd.read_csv('/content/drive/MyDrive/database/1) iris.csv')"],"metadata":{"id":"8Q8xV9DxoMDf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Overview\n","We load the house price dataset to explore its structure, data types, and check for missing values"],"metadata":{"id":"FHeLCSsWobMu"}},{"cell_type":"code","source":["#Displaying Firt Couple of Rows\n","df.head()"],"metadata":{"id":"1ND2YFGVoZuv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Data Info\n","df.info()"],"metadata":{"id":"I1gGBiuxo7JI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Data Describe\n","df.describe()"],"metadata":{"id":"wchGCp86o-Jo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Dataset Summary**\n","* Rows: 150\n","* Columns: 6\n","* Each row = 1 Iris flower record\n","* Features: 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'\n","* The 'Id' column is an identifier and will be dropped.\n","* The 'Species' column indicates the true species of the Iris flower. Since K-Means is unsupervised, we will not use this for training but can use it later to evaluate how well the clusters correspond to the actual species.\n","* Missing values: None.\n"],"metadata":{"id":"TRP1v0S8pGRf"}},{"cell_type":"markdown","source":["#  Data Preprocessing\n","We prepare the data for clustering. This involves selecting the features to be used for clustering and potentially scaling them.\n"],"metadata":{"id":"sH_PM858pT6Q"}},{"source":["# Selecting features for clustering by name\n","# We will use 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthM'\n","# Correcting the column names to match the actual DataFrame columns (likely lowercase)\n","feature_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'] # Assuming lowercase based on typical iris datasets\n","\n","X = df[feature_columns].values # Selecting columns by name\n","\n","# The 'Id' column is not needed for clustering.\n","# The 'Species' column will be kept aside for later comparison.\n","# Make sure to check the column name in your actual dataframe - it might be capitalized 'Species'\n","# Correcting species column name lookup as well\n","try:\n","    species = df['species'] # Assuming lowercase based on typical iris datasets\n","except KeyError:\n","    # Fallback in case the column name is uppercase 'Species'\n","    species = df['Species']\n","\n","\n","print(\"Features (X) selected. Shape:\", X.shape)\n","\n","# Feature Scaling (Optional but often recommended for K-Means)\n","# K-Means is distance-based, so features on different scales can disproportionately influence results.\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","print(\"\\nFirst 5 rows of scaled features:\")\n","print(X_scaled[:5])"],"cell_type":"code","metadata":{"id":"BC4fPdrOq3ox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data visualization"],"metadata":{"id":"G1_R6B3FrG4o"}},{"cell_type":"code","source":["# Finding the optimum number of clusters for k-means classification\n","wcss = [] # Within-cluster sum of squares\n","\n","for i in range(1, 11): # Testing k from 1 to 10\n","    kmeans = KMeans(n_clusters = i, init = 'k-means++',\n","                    max_iter = 300, n_init = 10, random_state = 42) # n_init=10 is default, explicitly stated\n","    kmeans.fit(X_scaled)\n","    wcss.append(kmeans.inertia_)\n","\n","# Plotting the results onto a line graph, allowing us to observe 'The elbow'\n","plt.figure(figsize=(8, 5))\n","plt.plot(range(1, 11), wcss, marker='o')\n","plt.title('The Elbow Method')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('WCSS (Inertia)') # Within cluster sum of squares\n","plt.xticks(range(1, 11))\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"8Tssek8yq9Iv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation**\n","\n","The plot shows that the WCSS decreases as 'k' increases. The \"elbow\" point appears to be around k=3. After k=3, the decrease in WCSS becomes less significant. Therefore, we will choose 3 clusters for our K-Means model, which also aligns with the known number of species in the Iris dataset."],"metadata":{"id":"kNDlUOzdrDEf"}},{"cell_type":"markdown","source":["# K-Means Model Training\n","Now, we apply the K-Means algorithm with the chosen number of clusters (k=3)."],"metadata":{"id":"UJfYMYchrVa_"}},{"cell_type":"code","source":["# Applying kmeans to the dataset with the optimal number of clusters\n","optimal_k = 3\n","kmeans = KMeans(n_clusters = optimal_k, init = 'k-means++',\n","                max_iter = 300, n_init = 10, random_state = 42)\n","y_kmeans = kmeans.fit_predict(X_scaled) # Fit the model and predict cluster labels\n","\n","print(\"Cluster labels assigned to each data point:\")\n","print(y_kmeans[:10]) # Displaying first 10 cluster assignments\n","\n","print(\"\\nCluster Centers (centroids):\")\n","print(scaler.inverse_transform(kmeans.cluster_centers_)) # Display centroids in original scale\n"],"metadata":{"id":"ogzBOcmsrXFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualising the clusters - On the first two columns (Sepal Length vs Sepal Width)\n","plt.figure(figsize=(10, 7))\n","\n","# Scatter plot for data points, colored by their assigned cluster\n","plt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1],\n","            s = 80, c = 'purple', label = 'Cluster 1 (e.g., Iris-setosa)')\n","plt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1],\n","            s = 80, c = 'orange', label = 'Cluster 2 (e.g., Iris-versicolour)')\n","plt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1],\n","            s = 80, c = 'green', label = 'Cluster 3 (e.g., Iris-virginica)')\n","\n","# Plotting the centroids of the clusters\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","            s = 150, c = 'red', label = 'Centroids', marker='X', edgecolor='black')\n","\n","plt.title('Clusters of Iris Flowers (Sepal Length vs Sepal Width - Scaled)')\n","plt.xlabel('Sepal Length (Standardized)')\n","plt.ylabel('Sepal Width (Standardized)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"7t7xuffGrasY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualising the clusters - On Petal Length vs Petal Width\n","plt.figure(figsize=(10, 7))\n","\n","# Scatter plot for data points, colored by their assigned cluster\n","plt.scatter(X_scaled[y_kmeans == 0, 2], X_scaled[y_kmeans == 0, 3],\n","            s = 80, c = 'purple', label = 'Cluster 1')\n","plt.scatter(X_scaled[y_kmeans == 1, 2], X_scaled[y_kmeans == 1, 3],\n","            s = 80, c = 'orange', label = 'Cluster 2')\n","plt.scatter(X_scaled[y_kmeans == 2, 2], X_scaled[y_kmeans == 2, 3],\n","            s = 80, c = 'green', label = 'Cluster 3')\n","\n","# Plotting the centroids of the clusters\n","plt.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3],\n","            s = 150, c = 'red', label = 'Centroids', marker='X', edgecolor='black')\n","\n","plt.title('Clusters of Iris Flowers (Petal Length vs Petal Width - Scaled)')\n","plt.xlabel('Petal Length (Standardized)')\n","plt.ylabel('Petal Width (Standardized)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"P4aloQYarfyf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explanation:\n","The scatter plots visualize the data points colored by their assigned clusters and the cluster centroids.\n","\n","The first plot shows clustering based on Sepal Length and Sepal Width.\n","\n","The second plot shows clustering based on Petal Length and Petal Width. Petal dimensions often show better separation for Iris species.\n","\n","The K-Means algorithm has grouped the Iris flowers into three distinct clusters. We can observe that the clusters are relatively well-separated, especially when visualized using petal dimensions."],"metadata":{"id":"tBeCYvtTrkW_"}},{"cell_type":"markdown","source":["# Comparing with Actual Species (Optional Evaluation)\n","Although K-Means is unsupervised, we can compare the resulting clusters with the actual species labels to see how well the algorithm has grouped similar flowers."],"metadata":{"id":"i3reIx4Trpav"}},{"source":["try:\n","    species = df['species'] # Assuming lowercase based on typical iris datasets\n","except KeyError:\n","    # Fallback in case the column name is uppercase 'Species'\n","    species = df['Species']"],"cell_type":"code","metadata":{"id":"70k591QDr5BI"},"execution_count":null,"outputs":[]},{"source":["# Add the predicted cluster labels to the original DataFrame (optional, for inspection)\n","df['Cluster'] = y_kmeans\n","\n","# Map actual species to numerical labels for easier comparison if needed\n","# Use the lowercase 'species' column name which was confirmed to exist\n","species_map = {species_name: i for i, species_name in enumerate(df['species'].unique())}\n","df['Species_Encoded'] = df['species'].map(species_map) # Also use lowercase here\n","\n","# Create a cross-tabulation (confusion matrix like) to see the distribution\n","# of actual species within each predicted cluster.\n","# Note: Cluster labels (0, 1, 2) from K-Means might not directly correspond to a specific species_encoded label.\n","# We need to see which cluster predominantly contains which species.\n","\n","contingency_matrix = pd.crosstab(df['species'], df['Cluster']) # Use lowercase 'species' here\n","print(\"\\nContingency Matrix (Actual Species vs. Predicted Cluster):\")\n","print(contingency_matrix)\n","\n","# Visualize the contingency matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(contingency_matrix, annot=True, fmt='d', cmap='Blues')\n","plt.title('Actual Species vs. Predicted Cluster')\n","plt.xlabel('Predicted Cluster')\n","plt.ylabel('Actual Species')\n","plt.show()"],"cell_type":"code","metadata":{"id":"uAJgT3q_sGvo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interpretation of Contingency Matrix:**\n","* **Cluster 0:** Contains 50 instances, all of which are 'Iris-setosa'. This cluster perfectly identifies 'Iris-setosa'.\n","* **Cluster 1:** Contains 48 'Iris-versicolor' and 14 'Iris-virginica'. This cluster primarily captures 'Iris-versicolor' but has some misclassifications of 'Iris-virginica'.\n","* **Cluster 2:** Contains 2 'Iris-versicolor' and 36 'Iris-virginica'. This cluster primarily captures 'Iris-virginica' but has some misclassifications of 'Iris-versicolor'.\n","\n","This comparison shows that K-Means has done a reasonably good job of separating the species, especially 'Iris-setosa'. The overlap between 'Iris-versicolor' and 'Iris-virginica' is a known characteristic of the Iris dataset, and the clustering reflects this.\n"],"metadata":{"id":"2K3EmcmYsMxp"}},{"cell_type":"markdown","source":["## Conclusion\n","In this task, we successfully applied the K-Means clustering algorithm to the Iris dataset.\n","1. We loaded and explored the dataset.\n","2. We preprocessed the data by selecting relevant features and applying standard scaling.\n","3. The Elbow Method suggested k=3 as the optimal number of clusters, which aligns with the known three species in the dataset.\n","4. The K-Means model was trained, and data points were assigned to one of the three clusters.\n","5. Visualizations of the clusters (using Sepal and Petal dimensions) showed distinct groupings.\n","6. A comparison with the actual species labels (via a contingency matrix) indicated that the algorithm effectively separated 'Iris-setosa' and provided a good, though not perfect, separation between 'Iris-versicolor' and 'Iris-virginica'.\n","\n","K-Means clustering proved to be a useful technique for discovering underlying group structures within the Iris dataset without prior knowledge of the species labels.\n"],"metadata":{"id":"-Ihd73h-sSh3"}}]}